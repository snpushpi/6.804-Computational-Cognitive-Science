{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plot_Generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKDob8hnKBpV1ad5hiWb1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/6.804-Computational-Cognitive-Science/blob/master/Plot_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW9qCct6leaK"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import scipy.stats"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQq1wVoYo1iJ"
      },
      "source": [
        "state_set = {'A','B','C','D'}\n",
        "mu_dict = {'A':.2,'B':.4,'C':.6,'D':.8}\n",
        "sigma = 0.1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3dtD6V6pFUe"
      },
      "source": [
        "def weight_based_sampling(S): #[[state,weight]]\n",
        "    states=[e[0] for e in S] \n",
        "    weights = [e[1] for e in S]\n",
        "    state =  np.random.choice(states,p=weights) #states [[][]]\n",
        "    weight = None\n",
        "    for elt in S:\n",
        "        if elt[0]==state:\n",
        "            weight = elt[1]\n",
        "    return state,weight"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1CEWbVmpGFa"
      },
      "source": [
        "def weight_calculate(S):\n",
        "    weight_dict = {'A':0,'B':0,'C':0,'D':0}\n",
        "    for state,weight in S:\n",
        "        weight_dict[state]+=weight\n",
        "    return weight_dict"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6EttbHZpVjC"
      },
      "source": [
        "PF class goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvKCVjBEpYCN"
      },
      "source": [
        "class Particle_Filter():\n",
        "\n",
        "    def __init__(self,particle_num,r):\n",
        "        self.n = particle_num\n",
        "        self.S = [[random.choice(tuple(state_set)),1/particle_num] for i in range(particle_num)]\n",
        "        self.r = r\n",
        "        self.prediction = None\n",
        "        self.similarity = None\n",
        "        self.weight_dict = None\n",
        "\n",
        "    def update(self,observation, human_observation):\n",
        "        '''self.S gets updated at each trial.'''\n",
        "        S_new = [] \n",
        "        eta = 0 \n",
        "        for i in range(self.n):\n",
        "            state,weight = weight_based_sampling(self.S) #a particle \n",
        "            x1 = random.uniform(0,1) \n",
        "            if x1<self.r: #change state - (1-e**(-r*k)) =>\n",
        "                new_set = state_set-{state}\n",
        "                state= random.choice(tuple(new_set))\n",
        "            new_weight = scipy.stats.norm(mu_dict[state],sigma).pdf(observation)\n",
        "            eta+= new_weight\n",
        "            S_new.append([state,new_weight])\n",
        "        S_new = [[elt[0],elt[1]/eta] for elt in S_new]\n",
        "        self.weight_dict = weight_calculate(S_new)\n",
        "        self.prediction = max(self.weight_dict, key=self.weight_dict.get)\n",
        "        self.similarity = self.weight_dict[human_observation]\n",
        "        self.S = S_new\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbHtjGhT1y4n"
      },
      "source": [
        "Normal PF run goes here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkfcv9E11dD"
      },
      "source": [
        "def normal_PF_run(particle_number,observation_list,human_inference_list,r):\n",
        "    PF = Particle_Filter(particle_number,r)\n",
        "    model_inference = []\n",
        "    for i in range(len(observation_list)):\n",
        "        PF.update(observation_list[i],human_inference_list[i])\n",
        "        model_inference.append([PF.prediction, PF.similarity])\n",
        "    return model_inference"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLXtSc4LrmVN"
      },
      "source": [
        "Nested PF helper function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xQApXRsrocW"
      },
      "source": [
        "def most_probable_state_prediction(particle_filter_list):\n",
        "    '''Each element in this list is a particle filter itself. This function returns teh most weighed state of all the particle \n",
        "    filters. Recall that particle filters themselves are weighed too, so we keep that in mind in the computation too\n",
        "    '''\n",
        "    #weight check \n",
        "    check_weight = 0\n",
        "    for particle_filter, weight in particle_filter_list:\n",
        "        check_weight+=weight  \n",
        "    weight_dictionary = {'A':0,'B':0,'C':0,'D':0}\n",
        "    for particle_filter,particle_filter_weight in particle_filter_list:\n",
        "        for elt in weight_dictionary:\n",
        "            weight_dictionary[elt]+= particle_filter.weight_dict[elt]*particle_filter_weight\n",
        "    max_prob_state = max(weight_dictionary, key=weight_dictionary.get)\n",
        "    return max_prob_state, weight_dictionary "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu50jfAupdu8"
      },
      "source": [
        "Nested PF run goes here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVv2ZsKApf_2"
      },
      "source": [
        "def nested_PF_run(particle_filter_num, observation_list, human_prediction_list, particle_num, r):\n",
        "    #main challenge - how to weight the particle filters themselves?\n",
        "    #working on the base that the particle filters whoch are more consistent with the human rpediction should be weghed high\n",
        "    #The way I do it is the following - \n",
        "    #After any trial, the weight of a particle filter is the probability with which it is generating the observant's prediction \n",
        "    particle_filter_list = [[Particle_Filter(particle_num,r),1/particle_filter_num] for i in range(particle_filter_num)]\n",
        "    model_prediction = []\n",
        "    for i in range(len(observation_list)):\n",
        "        eta = 0\n",
        "        H_new = []\n",
        "        particle_filter_keeper = []\n",
        "        #model_prediction.append(most_probable_state_prediction(particle_filter_list))\n",
        "        for particle_filter,weight in particle_filter_list:\n",
        "            particle_filter.update(observation_list[i],human_prediction_list[i])\n",
        "            particle_filter_keeper.append([particle_filter,weight])\n",
        "            weight = particle_filter.weight_dict[human_prediction_list[i]] \n",
        "            H_new.append([particle_filter,weight])\n",
        "            eta+=weight\n",
        "        most_prob_state, weight_dict = most_probable_state_prediction(particle_filter_keeper)\n",
        "        model_prediction.append([most_prob_state,weight_dict[human_prediction_list[i]]])\n",
        "        H_new = [[elt[0],elt[1]/eta] for elt in H_new]\n",
        "        #At this point, we will get the model prediction, which we are actually getting from the weighed particle filters,\n",
        "        #In other words, each particle filter is consistent with human prediction and the more it is consistent, the higherweight it gets\n",
        "        #Now to get the model prediction as a whole, or the prediction from researcher filter would be the weighed prediction from all partcile filters\n",
        "        # and the state getting highest weight this way is the researcher's prediction. This role is being done by the most_probable_state_prediction\n",
        "        #H_New = [[particle_filter, weight]]\n",
        "        particle_filter_index_list = [[i,H_new[i][1]] for i in range(particle_filter_num)]\n",
        "        new_list = []\n",
        "        weight_sum = 0\n",
        "        for j in range(particle_filter_num):\n",
        "            index, weight = weight_based_sampling(particle_filter_index_list)\n",
        "            weight_sum+= weight\n",
        "            new_list.append([H_new[index][0],weight])\n",
        "        particle_filter_list =  [[elt[0],elt[1]/weight_sum] for elt in new_list]\n",
        "    return model_prediction"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NctpQf78qV6u"
      },
      "source": [
        "Normal PF similarity measure here -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvcXpCgktL3I"
      },
      "source": [
        "def normal_PF_accuracy_and_similarity(human_inference_list, particle_number, observation_list, actual_list, r):\n",
        "    '''This function returns both the task accuracy and human similarity measure of the normal\n",
        "    particle filter model'''\n",
        "    model_inference_list = normal_PF_run(particle_number,observation_list,human_inference_list,r)\n",
        "    human_similarity_measure = 0\n",
        "    task_counter = 0\n",
        "    for i in range(len(observation_list)):\n",
        "        human_similarity_measure+=model_inference_list[i][1]\n",
        "        if actual_list[i]==model_inference_list[i][0]:\n",
        "            task_counter+=1\n",
        "    return human_similarity_measure/len(observation_list),task_counter/len(observation_list)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0JaiqpHtWfS"
      },
      "source": [
        "Nested PF similarity measure here-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oKMEB2ItYGt"
      },
      "source": [
        "def nested_PF_accuracy_and_similarity(human_inference_list, particle_number, particle_filter_num, observation_list, actual_list, r):\n",
        "    '''This function returns both the task accuracy and human similarity measure of the normal\n",
        "    particle filter model'''\n",
        "    model_inference_list = nested_PF_run(particle_filter_num, observation_list, human_inference_list, particle_number, r)\n",
        "    human_similarity_measure = 0\n",
        "    task_counter = 0\n",
        "    for i in range(len(observation_list)):\n",
        "        human_similarity_measure+=model_inference_list[i][1]\n",
        "        if actual_list[i]==model_inference_list[i][0]:\n",
        "            task_counter+=1\n",
        "    return human_similarity_measure/len(observation_list),task_counter/len(observation_list)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff0EkXS7uBof"
      },
      "source": [
        "Synthetic data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL0s5G7gu7qM"
      },
      "source": [
        "def synthetic_data_generate(observation_list):\n",
        "    inference_list = []\n",
        "    for observation in observation_list:\n",
        "        best_state = 'A'\n",
        "        for state in state_set:\n",
        "            if scipy.stats.norm(mu_dict[state],sigma).pdf(observation)>scipy.stats.norm(mu_dict[best_state],sigma).pdf(observation):\n",
        "                best_state = state\n",
        "        inference_list.append(best_state)\n",
        "    return inference_list"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-CXokzbw-_f"
      },
      "source": [
        "Observation List generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs8ZuiD_xBTY"
      },
      "source": [
        "def data_generate(alpha,T):\n",
        "    '''Goal is generate T observations with state changing probability alpha.\n",
        "    Step 1: Set Z_0 as a random sample from state list.\n",
        "    Step 2: Repeat the following for T trials -\n",
        "          i) Z_i = Z_i-1\n",
        "          ii)Sample x randomly from [0,1]\n",
        "          iii) If x<alpha replace Z_i with random sample {A,B,C,D}/Z_i-1\n",
        "          iv)Sample stimulus y_i form a normal distribution with std sigma and mu_zi\n",
        "    '''\n",
        "    observation_list = []\n",
        "    Z = [None]*(T+1)\n",
        "    Z[0] = random.choice(tuple(state_set))\n",
        "    for i in range(1,T+1):\n",
        "        Z[i]=Z[i-1]\n",
        "        x = random.uniform(0,1)\n",
        "        if x<alpha:\n",
        "            new_set = state_set-{Z[i-1]}\n",
        "            Z[i]= random.choice(tuple(new_set))\n",
        "        observation_list.append(random.gauss(mu_dict[Z[i]],sigma))        \n",
        "    return observation_list,Z[1:]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrppeR8LvD_9"
      },
      "source": [
        "For each of the 103 synthetic subjects and each of the alphas, we run both normal PF and nested PF and calculate the similarity measure in ecah case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fRjBykGv6n6"
      },
      "source": [
        "alpha_list = [0.08,0.16,0.32]\n",
        "related_r = {0.08:0.1,0.16:.18,0.32:0.35}\n",
        "T = 40\n",
        "result_dict = {0.08:{'normal_PF':0,'nested_PF':0},0.16:{'normal_PF':0,'nested_PF':0},0.32:{'normal_PF':0,'nested_PF':0}}\n",
        "for alpha in alpha_list:\n",
        "    for i in range(103):\n",
        "        observation_list,actual_list = data_generate(alpha,T)\n",
        "        synthetic_inference_list = synthetic_data_generate(observation_list)\n",
        "        result_dict[alpha]['normal_PF'],_= normal_PF_accuracy_and_similarity(synthetic_inference_list, 1000, observation_list, actual_list, related_r[alpha])\n",
        "        result_dict[alpha]['nested_PF'],_= nested_PF_accuracy_and_similarity(synthetic_inference_list, 10, 100, observation_list, actual_list, related_r[alpha])\n",
        "        print(result_dict[alpha]['normal_PF'],result_dict[alpha]['nested_PF'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}