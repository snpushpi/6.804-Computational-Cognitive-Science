{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Particle_Filter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnWYDVe0ytany3vWp6nqfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/6.804-Computational-Cognitive-Science/blob/master/Particle_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1N42W3YV2Pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432224df-c233-490d-edf0-b45bc2b2cf62"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import scipy.stats\n",
        "from collections import Counter \n",
        "\n",
        "state_set = {'A','B','C','D'}\n",
        "mu_dict = {'A':.2,'B':.4,'C':.6,'D':.8}\n",
        "sigma = 0.1\n",
        "def data_generate(alpha,T):\n",
        "    '''Goal is generate T observations with state changing probability alpha.\n",
        "    Step 1: Set Z_0 as a random sample from state list.\n",
        "    Step 2: Repeat the following for T trials -\n",
        "          i) Z_i = Z_i-1\n",
        "          ii)Sample x randomly from [0,1]\n",
        "          iii) If x<alpha replace Z_i with random sample {A,B,C,D}/Z_i-1\n",
        "          iv)Sample stimulus y_i form a normal distribution with std sigma and mu_zi\n",
        "    '''\n",
        "    observation_list = []\n",
        "    Z = [None]*(T+1)\n",
        "    Z[0] = random.choice(tuple(state_set))\n",
        "    for i in range(1,T+1):\n",
        "        Z[i]=Z[i-1]\n",
        "        x = random.uniform(0,1)\n",
        "        if x<alpha:\n",
        "            new_set = state_set-{Z[i-1]}\n",
        "            Z[i]= random.choice(tuple(new_set))\n",
        "        observation_list.append(random.gauss(mu_dict[Z[i]],sigma))        \n",
        "    return observation_list,Z[1:]\n",
        "\n",
        "def weight_based_sampling(S): #[[state,weight]]\n",
        "    states=[e[0] for e in S]\n",
        "    weights = [e[1] for e in S]\n",
        "    state =  np.random.choice(states,p=weights)\n",
        "    weight = None\n",
        "    for elt in S:\n",
        "        if elt[0]==state:\n",
        "            weight = elt[1]\n",
        "    return state,weight\n",
        "\n",
        "def most_probable(S):\n",
        "    weight_dict = {'A':0,'B':0,'C':0,'D':0}\n",
        "    for state,weight in S:\n",
        "        weight_dict[state]+=weight\n",
        "    return max(weight_dict, key=weight_dict.get)\n",
        "\n",
        "def particle_filter(observation_list,r,particle_num,T):\n",
        "    '''1. Initialize a list (call it S_0) with a [particle_state,weight] as element(call each element [s_0,w_0]). Initially all weights are equal\n",
        "       2. Do thw follwing for trial 1 to trial T- \n",
        "          i) Create an empty list call S_i for storing new particles. Also initialize a normalization constant eta with 0\n",
        "          for i=1 to i=particle_num do the following - \n",
        "              ii) at trial j, consider the particle list S_{j-1} and sample a particle from this particle from this distribution based on weight w_{i-1}\n",
        "              iii) CAll the sampled particle from step ii x_i, now change state with probability alpha, stay in the same state with the rest.\n",
        "              Call this new particle x_i'\n",
        "              iv) w_i = P(ith observation|x_i'), eta+=w_i\n",
        "              v) S_i.append([x_i',w_i])\n",
        "          for i=1 to i=particle_num:\n",
        "              w_i/=w_i/eta\n",
        "    '''\n",
        "    S = [[random.choice(tuple(state_set)),1/particle_num] for i in range(particle_num)] #S=[[state,1/particle_num]]\n",
        "    prediction_list = []\n",
        "    for t in range(1,T+1): #\n",
        "        S_new = [] \n",
        "        eta = 0 \n",
        "        for i in range(particle_num):\n",
        "            state,weight = weight_based_sampling(S) #a particle \n",
        "            x1 = random.uniform(0,1) \n",
        "            if x1<r: #change state - (1-e**(-r*k)) =>\n",
        "                new_set = state_set-{state}\n",
        "                state= random.choice(tuple(new_set))\n",
        "            new_weight = scipy.stats.norm(mu_dict[state],sigma).pdf(observation_list[t-1])\n",
        "            eta+= new_weight\n",
        "            S_new.append([state,new_weight])\n",
        "        S_new = [[elt[0],elt[1]/eta] for elt in S_new]\n",
        "        prediction_list.append(most_probable(S_new))\n",
        "        S = S_new #S_t - S-\n",
        "    return prediction_list\n",
        "\n",
        "def count_accuracy(alpha,T,particle_num,r):\n",
        "    observation_list,actual_list = data_generate(alpha,T)\n",
        "    prediction_list = particle_filter(observation_list,r,particle_num,T)\n",
        "    count = 0\n",
        "    for i in range(len(observation_list)):\n",
        "        if prediction_list[i]==actual_list[i]:\n",
        "            count+=1\n",
        "    return count/T\n",
        "print(count_accuracy(0.08,1000,150,0.1))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.911\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}