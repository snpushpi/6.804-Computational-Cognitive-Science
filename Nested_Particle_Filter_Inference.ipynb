{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nested_Particle_Filter_Inference.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjLRRuQGh+HJhVQ7v0/cE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/6.804-Computational-Cognitive-Science/blob/master/Nested_Particle_Filter_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh5T0hgDWBzB"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvm0wgK_WKLi"
      },
      "source": [
        "state_set = {'A','B','C','D'}\n",
        "mu_dict = {'A':.2,'B':.4,'C':.6,'D':.8}\n",
        "sigma = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjtamOOyWNA7"
      },
      "source": [
        "def weight_based_sampling(S): #[[state,weight]]\n",
        "    states=[e[0] for e in S] \n",
        "    weights = [e[1] for e in S]\n",
        "    state =  np.random.choice(states,p=weights) #states [[][]]\n",
        "    weight = None\n",
        "    for elt in S:\n",
        "        if elt[0]==state:\n",
        "            weight = elt[1]\n",
        "    return state,weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU7Y-ABRWP4_"
      },
      "source": [
        "def weight_calculate(S):\n",
        "    weight_dict = {'A':0,'B':0,'C':0,'D':0}\n",
        "    for state,weight in S:\n",
        "        weight_dict[state]+=weight\n",
        "    return weight_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Ibd-GtW6ze"
      },
      "source": [
        "def most_probable_state_prediction(particle_filter_list):\n",
        "    '''Each element in this list is a particle filter itself. This function returns teh most weighed state of all the particle \n",
        "    filters. Recall that particle filters themselves are weighed too, so we keep that in mind in the computation too\n",
        "    '''\n",
        "    #weight check \n",
        "    check_weight = 0\n",
        "    for particle_filter, weight in particle_filter_list:\n",
        "        check_weight+=weight  \n",
        "    weight_dictionary = {'A':0,'B':0,'C':0,'D':0}\n",
        "    for particle_filter,particle_filter_weight in particle_filter_list:\n",
        "        for elt in weight_dictionary:\n",
        "            weight_dictionary[elt]+= particle_filter.weight_dict[elt]*particle_filter_weight\n",
        "    max_prob_state = max(weight_dictionary, key=weight_dictionary.get)\n",
        "    return max_prob_state, weight_dictionary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wDYRcHbWSbx"
      },
      "source": [
        "class Particle_Filter():\n",
        "\n",
        "    def __init__(self,particle_num,r):\n",
        "        self.n = particle_num\n",
        "        self.S = [[random.choice(tuple(state_set)),1/particle_num] for i in range(particle_num)]\n",
        "        self.r = r\n",
        "        self.prediction = None\n",
        "        self.weight_dict = None\n",
        "\n",
        "    def update(self,observation):\n",
        "        '''self.S gets updated at each trial.'''\n",
        "        S_new = [] \n",
        "        eta = 0 \n",
        "        for i in range(self.n):\n",
        "            state,weight = weight_based_sampling(self.S) #a particle \n",
        "            x1 = random.uniform(0,1) \n",
        "            if x1<self.r: #change state - (1-e**(-r*k)) =>\n",
        "                new_set = state_set-{state}\n",
        "                state= random.choice(tuple(new_set))\n",
        "            new_weight = scipy.stats.norm(mu_dict[state],sigma).pdf(observation)\n",
        "            eta+= new_weight\n",
        "            S_new.append([state,new_weight])\n",
        "        S_new = [[elt[0],elt[1]/eta] for elt in S_new]\n",
        "        self.weight_dict = weight_calculate(S_new)\n",
        "        self.prediction = max(self.weight_dict, key=self.weight_dict.get)\n",
        "        self.S = S_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3O-nYZ5WXpT"
      },
      "source": [
        "def nested_main(particle_filter_num, observation_list, human_prediction_list, particle_num, r):\n",
        "    #main challenge - how to weight the particle filters themselves?\n",
        "    #working on the base that the particle filters whoch are more consistent with the human rpediction should be weghed high\n",
        "    #The way I do it is the following - \n",
        "    #After any trial, the weight of a particle filter is the probability with which it is generating the observant's prediction \n",
        "    particle_filter_list = [[Particle_Filter(particle_num,r),1/particle_filter_num] for i in range(particle_filter_num)]\n",
        "    model_prediction = []\n",
        "    for i in range(len(observation_list)):\n",
        "        eta = 0\n",
        "        H_new = []\n",
        "        particle_filter_keeper = []\n",
        "        #model_prediction.append(most_probable_state_prediction(particle_filter_list))\n",
        "        for particle_filter,weight in particle_filter_list:\n",
        "            particle_filter.update(observation_list[i])\n",
        "            particle_filter_keeper.append([particle_filter,weight])\n",
        "            weight = particle_filter.weight_dict[human_prediction_list[i]] \n",
        "            H_new.append([particle_filter,weight])\n",
        "            eta+=weight\n",
        "        most_prob_state, weight_dict = most_probable_state_prediction(particle_filter_keeper)\n",
        "        model_prediction.append([most_prob_state,weight_dict[human_prediction_list[i]]])\n",
        "        H_new = [[elt[0],elt[1]/eta] for elt in H_new]\n",
        "        #At this point, we will get the model prediction, which we are actually getting from the weighed particle filters,\n",
        "        #In other words, each particle filter is consistent with human prediction and the more it is consistent, the higherweight it gets\n",
        "        #Now to get the model prediction as a whole, or the prediction from researcher filter would be the weighed prediction from all partcile filters\n",
        "        # and the state getting highest weight this way is the researcher's prediction. This role is being done by the most_probable_state_prediction\n",
        "        #H_New = [[particle_filter, weight]]\n",
        "        particle_filter_index_list = [[i,H_new[i][1]] for i in range(particle_filter_num)]\n",
        "        new_list = []\n",
        "        weight_sum = 0\n",
        "        for j in range(particle_filter_num):\n",
        "            index, weight = weight_based_sampling(particle_filter_index_list)\n",
        "            weight_sum+= weight\n",
        "            new_list.append([H_new[index][0],weight])\n",
        "        particle_filter_list =  [[elt[0],elt[1]/weight_sum] for elt in new_list]\n",
        "    return model_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKjcb2TIhp8t"
      },
      "source": [
        "def accuracy_and_similarity(human_inference_list, particle_number, particle_filter_num, observation_list, actual_list, r):\n",
        "    '''This function returns both the task accuracy and human similarity measure of the normal\n",
        "    particle filter model'''\n",
        "    model_inference_list = nested_main(particle_filter_num, observation_list, human_inference_list, particle_number, r)\n",
        "    human_similarity_measure = 0\n",
        "    task_counter = 0\n",
        "    for i in range(len(observation_list)):\n",
        "        human_similarity_measure+=model_inference_list[i][1]\n",
        "        if actual_list[i]==model_inference_list[i][0]:\n",
        "            task_counter+=1\n",
        "    return human_similarity_measure/len(observation_list),task_counter/len(observation_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWGAsTk-oTWP"
      },
      "source": [
        "def data_generate(alpha,T):\n",
        "    '''Goal is generate T observations with state changing probability alpha.\n",
        "    Step 1: Set Z_0 as a random sample from state list.\n",
        "    Step 2: Repeat the following for T trials -\n",
        "          i) Z_i = Z_i-1\n",
        "          ii)Sample x randomly from [0,1]\n",
        "          iii) If x<alpha replace Z_i with random sample {A,B,C,D}/Z_i-1\n",
        "          iv)Sample stimulus y_i form a normal distribution with std sigma and mu_zi\n",
        "    '''\n",
        "    observation_list = []\n",
        "    Z = [None]*(T+1)\n",
        "    Z[0] = random.choice(tuple(state_set))\n",
        "    for i in range(1,T+1):\n",
        "        Z[i]=Z[i-1]\n",
        "        x = random.uniform(0,1)\n",
        "        if x<alpha:\n",
        "            new_set = state_set-{Z[i-1]}\n",
        "            Z[i]= random.choice(tuple(new_set))\n",
        "        observation_list.append(random.gauss(mu_dict[Z[i]],sigma))        \n",
        "    return observation_list,Z[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGSZq5rdqFid"
      },
      "source": [
        "observation_list,actual_list = data_generate(0.16,20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVYrPurtTYZ"
      },
      "source": [
        "observation_list = [0.2876251534356482,\n",
        " 0.3093255562034109,\n",
        " 0.48407616095419964,\n",
        " 0.4837637156934066,\n",
        " 0.3429542790801123,\n",
        " 0.8145469658341187,\n",
        " 0.5052333561135628,\n",
        " 0.566893813089657,\n",
        " 0.5190551990406111,\n",
        " 0.6756708844580039,\n",
        " 0.5941568333778058,\n",
        " 0.6689426081101612,\n",
        " 0.08007089954529747,\n",
        " 0.24591174989381062,\n",
        " 0.13612431798914626,\n",
        " 0.16855115278807364,\n",
        " 0.02849659237553981,\n",
        " 0.2997500771551358,\n",
        " 0.3786856816409396,\n",
        " 0.17848722367921957]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxhvtp2ktW72"
      },
      "source": [
        "actual_list = ['B',\n",
        " 'B',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'C',\n",
        " 'D',\n",
        " 'A',\n",
        " 'A',\n",
        " 'A',\n",
        " 'A',\n",
        " 'A',\n",
        " 'A',\n",
        " 'A',\n",
        " 'A']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mWJ98c6ta1L",
        "outputId": "4bbad3c9-96e0-4b3c-d2f4-e43b4f5c54fc"
      },
      "source": [
        "human_inference_list = ['B','B','C','C','C','C','C','D','D','D','D','D','A','A',\n",
        " 'A','A','A','A','B','B']\n",
        "particle_number = 5\n",
        "r = 0.2\n",
        "particle_filter_num = 200\n",
        "accuracy_and_similarity(human_inference_list, particle_number, particle_filter_num, observation_list, actual_list, r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.45047769340560145, 0.65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}